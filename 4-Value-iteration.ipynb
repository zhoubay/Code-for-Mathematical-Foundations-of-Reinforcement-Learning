{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建世界模型\n",
    "1. state space: 状态空间，包括所有可能的状态\n",
    "2. action space: 动作空间，包括所有可能的动作，默认包括`up`, `down`, `left`, `right`以及`still`五个动作\n",
    "3. p(r | s, a): 状态-动作转移概率，即在状态s下执行动作a之后，环境给出的奖励r的概率，这里的奖励r是实际上来自于所有可能的s'的奖励的期望值\n",
    "4. p(s' | s, a): 状态转移概率，即在状态s下执行动作a之后，环境转移到状态s'的概率。可以越界，越界的reward设置为-100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grids import build_models, actions, plot_values_and_policy, plot_values_and_policy_gif\n",
    "# 定义网格参数\n",
    "grid_size = 5\n",
    "\n",
    "target_areas = [(3, 2)]  # 终止和危险状态\n",
    "forbidden_areas = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)] # 禁止状态\n",
    "\n",
    "states, p_r, p_s_prime = build_models(grid_size, target_areas, forbidden_areas, success_prob=1)\n",
    "print(len(states), states[:2])\n",
    "v_initial = {s: 0 for s in states}\n",
    "p_initial = {s: {\"still\": 1.0} for s in states}\n",
    "plot_values_and_policy(\n",
    "    value_dict=v_initial, \n",
    "    policy_dict=p_initial, \n",
    "    target_cells=target_areas, \n",
    "    forbidden_cells=forbidden_areas, \n",
    "    title=\"Initial State Value Function\")\n",
    "print(p_r)\n",
    "print(p_s_prime)\n",
    "print(p_r[(0, 0)])\n",
    "print(p_s_prime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def policy_evaluation(states, policy, p_r, p_s_prime, gamma, threshold=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    策略评估：计算当前策略下的状态价值函数\n",
    "    \"\"\"\n",
    "    v = {s: 0.0 for s in states}\n",
    "    for _ in range(max_iter):\n",
    "        delta = 0\n",
    "        value_new = {}\n",
    "        for s in states:\n",
    "            total = 0\n",
    "            # 遍历所有动作（根据策略的分布）\n",
    "            for a in policy[s]:\n",
    "                if policy[s][a] == 0:\n",
    "                    continue  # 概率为0的动作可直接跳过\n",
    "                # ------------------------------------------------------------------------------------\n",
    "                # 通过计算每一个action的总和来计算v_new(s)\n",
    "                # v_new(s) = ∑_a policy(s, a) * (∑_r p ( r | s, a) * r + gamma * ∑_{s'} p (s' | s, a) * v(s'))\n",
    "                # 建议使用列表推导来实现下面的代码\n",
    "                # 参考信息：https://docs.python.org/zh-cn/3.13/tutorial/datastructures.html#list-comprehensions\n",
    "                # 先通过p_r计算expected_r，再通过p_s_prime计算expected_next_v，累积加权到total\n",
    "                # 最后在循环外更新value_new[s]\n",
    "                # Expected code: ~3 lines\n",
    "                # ------------------------------------------------------------------------------------\n",
    "                # 计算预期奖励\n",
    "                expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "                # 计算预期下一状态价值\n",
    "                expected_next_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "                # 累积加权值\n",
    "                total += policy[s][a] * (expected_r + gamma * expected_next_v)\n",
    "                # ------------------------------------------------------------------------------------\n",
    "                # End of code snippet\n",
    "                # ------------------------------------------------------------------------------------\n",
    "            value_new[s] = total\n",
    "        delta = (sum((v[s] - value_new[s]) ** 2 for s in states)) ** 0.5\n",
    "        # 判断是否收敛\n",
    "        if delta < threshold:\n",
    "            break\n",
    "        v = value_new.copy()\n",
    "    return v\n",
    "\n",
    "def policy_improvement(states, actions, v, p_r, p_s_prime, gamma):\n",
    "    \"\"\"\n",
    "    策略改进：根据当前值函数生成新策略\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    for s in states:\n",
    "        new_policy[s] = {}\n",
    "        max_q = -float('inf')\n",
    "        argmax_q_a = None\n",
    "        # 遍历所有可能的动作\n",
    "        for a in actions:\n",
    "            expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "            expected_next_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "            q = expected_r + gamma * expected_next_v\n",
    "            max_q = max(max_q, q)\n",
    "            argmax_q_a = a if q == max_q else argmax_q_a\n",
    "        # 生成确定性策略（当前最优动作的概率为1）\n",
    "        new_policy[s][argmax_q_a] = 1.0\n",
    "    return new_policy\n",
    "\n",
    "def policy_iteration(states, actions, p_r, p_s_prime, gamma, initial_policy=None, threshold=1e-5, max_iter=1000):\n",
    "    # 初始化随机策略（均匀分布）\n",
    "    if initial_policy is None:\n",
    "        initial_policy = {s: {\"still\": 1.0} for s in states}\n",
    "    \n",
    "    policy_k = initial_policy.copy()\n",
    "    v_policy_k_minus_1 = None\n",
    "    for k in range(max_iter):\n",
    "        # 1. 策略评估\n",
    "        v_policy_k = policy_evaluation(states, policy_k, p_r, p_s_prime, gamma)\n",
    "        # 2. 策略改进\n",
    "        policy_k_plus_1 = policy_improvement(states, actions, v_policy_k, p_r, p_s_prime, gamma)\n",
    "        # 检查策略是否稳定\n",
    "        if v_policy_k_minus_1 is not None:\n",
    "            delta_v = sum((v_policy_k[s] - v_policy_k_minus_1[s]) ** 2 for s in states) ** 0.5\n",
    "        else:\n",
    "            delta_v = float('inf')\n",
    "        if delta_v < threshold:\n",
    "            break\n",
    "        v_policy_k_minus_1 = v_policy_k.copy()\n",
    "        policy_k = policy_k_plus_1.copy()\n",
    "    return policy_k, v_policy_k\n",
    "gamma = 0.9\n",
    "optimal_policy, optimal_v = policy_iteration(states, actions, p_r, p_s_prime, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values_and_policy(\n",
    "    value_dict=optimal_v,\n",
    "    policy_dict=optimal_policy,\n",
    "    forbidden_cells=forbidden_areas,\n",
    "    target_cells=target_areas,\n",
    "    title=\"State Value and Policy at Final Iteration\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_values_and_policy_gif(\n",
    "    v_history,\n",
    "    p_history,\n",
    "    forbidden_areas,\n",
    "    target_areas,\n",
    "    gif_save_path='./value_iteration.gif'\n",
    ")\n",
    "plot_values_and_policy(\n",
    "    value_dict=v_history[-1],\n",
    "    policy_dict=p_history[-1],\n",
    "    forbidden_cells=forbidden_areas,\n",
    "    target_cells=target_areas,\n",
    "    title=\"State Value and Policy at Final Iteration\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Invoke the workflow again to ensure the probability model respects bounds and prevents KeyErrors\n",
    "grid_size = 4\n",
    "terminals = [(3, 3)]           # 终止状态（目标）\n",
    "forbiddens = [(1, 1), (2, 2)]  # 危险区（惩罚 -10）\n",
    "actions = ['up', 'down', 'left', 'right', 'still']\n",
    "gamma = 0.9\n",
    "\n",
    "states, p_r, p_s_prime = build_models(grid_size, terminals, forbiddens, success_prob=0.8)\n",
    "\n",
    "# 改进的策略迭代调用\n",
    "optimal_policy, optimal_v = policy_iteration(states, actions, p_r, p_s_prime, gamma)\n",
    "\n",
    "# 打印最优策略和状态价值\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "for i in range(grid_size):\n",
    "    row = [optimal_v[(i, j)] for j in range(grid_size)]\n",
    "    print(\"  \".join([f\"{val:.1f}\" for val in row]))\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "action_symbols = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→', 'still': '⏺'}\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        s = (i, j)\n",
    "        if s in terminals:\n",
    "            row.append('G')      # 目标\n",
    "        elif s in forbiddens:\n",
    "            row.append('D')      # 危险\n",
    "        else:\n",
    "            best_action = [a for a, prob in optimal_policy[s].items() if prob == 1.0][0]\n",
    "            row.append(action_symbols[best_action])\n",
    "    print(\"  \".join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
