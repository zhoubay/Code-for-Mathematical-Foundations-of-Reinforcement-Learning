{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建世界模型\n",
    "1. state space: 状态空间，包括所有可能的状态\n",
    "2. action space: 动作空间，包括所有可能的动作，默认包括`up`, `down`, `left`, `right`以及`still`五个动作\n",
    "3. p(r | s, a): 状态-动作转移概率，即在状态s下执行动作a之后，环境给出的奖励r的概率，这里的奖励r是实际上来自于所有可能的s'的奖励的期望值\n",
    "4. p(s' | s, a): 状态转移概率，即在状态s下执行动作a之后，环境转移到状态s'的概率。可以越界，越界的reward设置为-100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grids import build_models, actions, plot_values_and_policy, plot_values_and_policy_gif\n",
    "# 定义网格参数\n",
    "grid_size = 5\n",
    "\n",
    "target_areas = [(3, 2)]  # 终止和危险状态\n",
    "forbidden_areas = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)] # 禁止状态\n",
    "\n",
    "states, p_r, p_s_prime = build_models(grid_size, target_areas, forbidden_areas, success_prob=1)\n",
    "print(len(states), states[:2])\n",
    "v_initial = {s: 0 for s in states}\n",
    "p_initial = {s: {\"still\": 1.0} for s in states}\n",
    "plot_values_and_policy(\n",
    "    value_dict=v_initial, \n",
    "    policy_dict=p_initial, \n",
    "    target_cells=target_areas, \n",
    "    forbidden_cells=forbidden_areas, \n",
    "    title=\"Initial State Value Function\")\n",
    "print(p_r)\n",
    "print(p_s_prime)\n",
    "print(p_r[(0, 0)])\n",
    "print(p_s_prime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_evaluation(states, policy, p_r, p_s_prime, gamma, initial_v, j_truncate):\n",
    "    \"\"\"\n",
    "    部分策略评估：固定迭代次数而非完全收敛\n",
    "    \n",
    "    Args:\n",
    "        initial_v: 初始价值函数 {s: float}\n",
    "        j_truncate (int): 最大迭代步数\n",
    "    \"\"\"\n",
    "    v = initial_v.copy()\n",
    "    for _ in range(j_truncate):\n",
    "        new_v = {}\n",
    "        for s in states:\n",
    "            total = 0.0\n",
    "            for a in policy[s]:\n",
    "                if policy[s][a] == 0:\n",
    "                    continue\n",
    "                # 计算期望奖励和下一状态价值\n",
    "                expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "                expected_next_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "                total += policy[s][a] * (expected_r + gamma * expected_next_v)\n",
    "            new_v[s] = total\n",
    "        v = new_v.copy()\n",
    "    return v\n",
    "\n",
    "\n",
    "def policy_improvement(states, actions, v, p_r, p_s_prime, gamma):\n",
    "    \"\"\"\n",
    "    策略改进：根据当前值函数生成新策略\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    for s in states:\n",
    "        new_policy[s] = {}\n",
    "        q_list = []\n",
    "        # 遍历所有可能的动作\n",
    "        for a in actions:\n",
    "            # ------------------------------------------------------------------------------------\n",
    "            # 计算q ( s, a )\n",
    "            # q ( s, a ) = expected_r ( s, a ) + gamma * expected_next_v ( s, a )\n",
    "            # 建议使用列表推导来实现下面的代码\n",
    "            # 参考信息：https://docs.python.org/zh-cn/3.13/tutorial/datastructures.html#list-comprehensions\n",
    "            # 先通过p_r计算expected_r，再通过p_s_prime计算expected_next_v，累积加权到q\n",
    "            # 将(q, a) 存入q_list\n",
    "            # Expected code: ~4 lines\n",
    "            # ------------------------------------------------------------------------------------\n",
    "            expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "            expected_next_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "            q = expected_r + gamma * expected_next_v\n",
    "            q_list.append((q, a))\n",
    "            # ------------------------------------------------------------------------------------\n",
    "            # End of code snippet\n",
    "            # ------------------------------------------------------------------------------------\n",
    "        argmax_q_a = max(q_list, key=lambda x: x[0])[1]\n",
    "        # 生成确定性策略（当前最优动作的概率为1）\n",
    "        new_policy[s][argmax_q_a] = 1.0\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def truncated_policy_iteration(states, actions, p_r, p_s_prime, gamma, \n",
    "                               j_truncate=3, initial_policy=None, \n",
    "                               threshold=1e-5, max_iter=1000, save_history=False):\n",
    "    \"\"\"\n",
    "    切割策略迭代主算法\n",
    "    \n",
    "    Args:\n",
    "        j_truncate: 策略评估阶段的固定迭代次数\n",
    "    \"\"\"\n",
    "    # 策略初始化（确保确定性策略）\n",
    "    policy_k = initial_policy if initial_policy is not None else {s: {'still': 1.0} for s in states}\n",
    "    \n",
    "    # 初始价值函数为0\n",
    "    v_k = {s: 0.0 for s in states}\n",
    "    prev_v_k = None\n",
    "    \n",
    "    # 历史记录\n",
    "    history = {'v_history': [], 'p_history': []} if save_history else None\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # 1. 策略评估（固定j_truncate步）\n",
    "        v_k = truncated_policy_evaluation(states, policy_k, p_r, p_s_prime, gamma, v_k, j_truncate)\n",
    "        \n",
    "        # 2. 策略改进（同标准策略迭代）\n",
    "        policy_next = policy_improvement(states, actions, v_k, p_r, p_s_prime, gamma)\n",
    "        \n",
    "        # 记录历史\n",
    "        if save_history:\n",
    "            history['v_history'].append(v_k.copy())\n",
    "            history['p_history'].append(policy_next.copy())\n",
    "            \n",
    "        # 判断价值函数是否收敛\n",
    "        if prev_v_k is not None:\n",
    "            delta = max(abs(v_k[s] - prev_v_k[s]) for s in states)\n",
    "            if delta < threshold:\n",
    "                break\n",
    "        prev_v_k = v_k.copy()\n",
    "        policy_k = policy_next.copy()\n",
    "    \n",
    "    result = {'v': v_k, 'p': policy_k}\n",
    "    if save_history:\n",
    "        result.update(history)\n",
    "    return result\n",
    "gamma = 0.9\n",
    "return_dict = truncated_policy_iteration(\n",
    "    states=states,\n",
    "    actions=actions,\n",
    "    p_r=p_r,\n",
    "    p_s_prime=p_s_prime,\n",
    "    gamma=gamma,\n",
    "    j_truncate=5,   # 控制评估步数（核心参数）\n",
    "    save_history=True\n",
    ")\n",
    "optimal_v = return_dict[\"v\"]\n",
    "optimal_p = return_dict[\"p\"]\n",
    "v_history = return_dict.get(\"v_history\", None)\n",
    "p_history = return_dict.get(\"p_history\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_values_and_policy(\n",
    "    value_dict=optimal_v,\n",
    "    policy_dict=optimal_p,\n",
    "    forbidden_cells=forbidden_areas,\n",
    "    target_cells=target_areas,\n",
    "    title=\"State Value and Policy at Final Iteration\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
