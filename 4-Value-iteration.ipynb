{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建世界模型\n",
    "1. state space: 状态空间，包括所有可能的状态\n",
    "2. action space: 动作空间，包括所有可能的动作，默认包括`up`, `down`, `left`, `right`以及`still`五个动作\n",
    "3. p(r | s, a): 状态-动作转移概率，即在状态s下执行动作a之后，环境给出的奖励r的概率，这里的奖励r是实际上来自于所有可能的s'的奖励的期望值\n",
    "4. p(s' | s, a): 状态转移概率，即在状态s下执行动作a之后，环境转移到状态s'的概率。可以越界，越界的reward设置为-100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grids import build_models, actions, plot_policy, plot_values\n",
    "\n",
    "# 定义网格参数\n",
    "grid_size = 5\n",
    "\n",
    "target_areas = [(3, 2)]  # 终止和危险状态\n",
    "forbidden_areas = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)] # 禁止状态\n",
    "\n",
    "states, p_r, p_s_prime = build_models(grid_size, target_areas, forbidden_areas, success_prob=1)\n",
    "v_initial = {s: 0 for s in states}\n",
    "plot_values(v_initial, \"Initial State Value Function\")\n",
    "print(p_r)\n",
    "print(p_s_prime)\n",
    "print(p_r[(0, 0)])\n",
    "print(p_s_prime.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def value_iteration(p_r, p_s_prime, v_init, states, target_areas, actions, gamma, threshold=1e-5, max_iter=100, save_history=False):\n",
    "    v = v_init.copy()\n",
    "    if save_history:\n",
    "        v_history = []\n",
    "    for _ in range(max_iter):\n",
    "        delta = 0\n",
    "        v_new = {}\n",
    "        for s in states:\n",
    "            if s in target_areas:\n",
    "                v_new[s] = 0  # 终止状态价值保持0（已终止）\n",
    "                continue\n",
    "            max_q = -float('inf')\n",
    "            for a in actions:\n",
    "                expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "                expected_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "                q = expected_r + gamma * expected_v\n",
    "                max_q = max(max_q, q)\n",
    "            v_new[s] = max_q\n",
    "            delta = max(delta, abs(v_new[s] - v.get(s, 0)))\n",
    "        v = v_new.copy()\n",
    "        if save_history:\n",
    "            v_history.append(v.copy())\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    # 提取策略\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        if s in target_areas:\n",
    "            policy[s] = 'still'\n",
    "            continue\n",
    "        best_action = None\n",
    "        max_q = -float('inf')\n",
    "        for a in actions:\n",
    "            expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "            expected_v = sum(prob * v[s_prime] for s_prime, prob in p_s_prime[s][a].items())\n",
    "            q = expected_r + gamma * expected_v\n",
    "            if q > max_q:\n",
    "                max_q = q\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return_dict = {\n",
    "        'v': v,\n",
    "        'policy': policy,\n",
    "    }\n",
    "    if save_history:\n",
    "        return_dict['v_history'] = v_history\n",
    "    return return_dict\n",
    "\n",
    "# 运行算法\n",
    "gamma = 0.9\n",
    "return_dict = value_iteration(p_r, p_s_prime, v_initial, states, target_areas, actions, gamma, save_history=True)\n",
    "v_optimal = return_dict['v']\n",
    "policy_optimal = return_dict['policy']\n",
    "history = return_dict['v_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "\n",
    "file_paths = []\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    \n",
    "    # 画图\n",
    "    for idx, v_h in enumerate(history):\n",
    "        temp_file = os.path.join(tmpdir, \"plot_\" + str(idx) + \".png\")\n",
    "        plot_values(v_h, \"State Value Function at Iteration \" + str(idx+1), save_path=temp_file)\n",
    "        file_paths.append(temp_file)\n",
    "    images = [Image.open(f) for f in file_paths]\n",
    "\n",
    "# 保存为GIF（设置帧间隔和循环次数）\n",
    "gif_save_path = \"value_iteration.gif\"\n",
    "images[0].save(\n",
    "    gif_save_path,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    duration=500,  # 每帧持续时间（单位：毫秒）\n",
    "    loop=0,        # 0表示无限循环\n",
    "    optimize=True\n",
    ")\n",
    "plot_values(v_optimal, \"Optimal State Value Function\")\n",
    "plot_policy(policy_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def policy_evaluation(states, policy, p_r, p_s_prime, gamma, threshold=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    策略评估：计算当前策略下的状态价值函数\n",
    "    \"\"\"\n",
    "    v = {s: 0.0 for s in states}\n",
    "    for _ in range(max_iter):\n",
    "        delta = 0\n",
    "        v_new = {}\n",
    "        for s in states:\n",
    "            total = 0\n",
    "            # 遍历所有动作（根据策略的分布）\n",
    "            for a in policy[s]:\n",
    "                if policy[s][a] == 0:\n",
    "                    continue  # 概率为0的动作可直接跳过\n",
    "                # 计算预期奖励\n",
    "                expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "                # 计算预期下一状态价值\n",
    "                expected_next_v = sum(prob * v[bounce_back(s_prime, grid_size)] for s_prime, prob in p_s_prime[s][a].items())\n",
    "                # 累积加权值\n",
    "                total += policy[s][a] * (expected_r + gamma * expected_next_v)\n",
    "            v_new[s] = total\n",
    "            delta = max(delta, abs(v_new[s] - v[s]))\n",
    "        # 判断是否收敛\n",
    "        if delta < threshold:\n",
    "            break\n",
    "        v = v_new.copy()\n",
    "    return v\n",
    "\n",
    "def policy_improvement(states, actions, v, p_r, p_s_prime, gamma):\n",
    "    \"\"\"\n",
    "    策略改进：根据当前值函数生成新策略\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    for s in states:\n",
    "        q_values = {}\n",
    "        # 遍历所有可能的动作\n",
    "        for a in actions:\n",
    "            expected_r = sum(prob * r for r, prob in p_r[s][a].items())\n",
    "            expected_next_v = sum(prob * v[bounce_back(s_prime, grid_size)] for s_prime, prob in p_s_prime[s][a].items())\n",
    "            q = expected_r + gamma * expected_next_v\n",
    "            q_values[a] = q\n",
    "        # 找到最大Q值的动作（随机选择平局）\n",
    "        max_q = max(q_values.values())\n",
    "        best_actions = [a for a in q_values if q_values[a] == max_q]\n",
    "        best_action = random.choice(best_actions)\n",
    "        # 生成确定性策略（当前最优动作的概率为1）\n",
    "        new_policy[s] = {a: 0.0 for a in actions}\n",
    "        new_policy[s][best_action] = 1.0\n",
    "    return new_policy\n",
    "\n",
    "def policy_iteration(states, actions, p_r, p_s_prime, gamma, initial_policy=None, threshold=1e-5, max_iter=1000):\n",
    "    # 初始化随机策略（均匀分布）\n",
    "    if initial_policy is None:\n",
    "        initial_policy = {s: {a: 1/len(actions) for a in actions} for s in states}\n",
    "    \n",
    "    policy = initial_policy.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # 1. 策略评估\n",
    "        v = policy_evaluation(states, policy, p_r, p_s_prime, gamma)\n",
    "        # 2. 策略改进\n",
    "        new_policy = policy_improvement(states, actions, v, p_r, p_s_prime, gamma)\n",
    "        # 检查策略是否稳定\n",
    "        if all(np.allclose(list(policy[s].values()), list(new_policy[s].values())) for s in states):\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Invoke the workflow again to ensure the probability model respects bounds and prevents KeyErrors\n",
    "grid_size = 4\n",
    "terminals = [(3, 3)]           # 终止状态（目标）\n",
    "forbiddens = [(1, 1), (2, 2)]  # 危险区（惩罚 -10）\n",
    "actions = ['up', 'down', 'left', 'right', 'still']\n",
    "gamma = 0.9\n",
    "\n",
    "states, p_r, p_s_prime = build_models(grid_size, terminals, forbiddens, success_prob=0.8)\n",
    "\n",
    "# 改进的策略迭代调用\n",
    "optimal_policy, optimal_v = policy_iteration(states, actions, p_r, p_s_prime, gamma)\n",
    "\n",
    "# 打印最优策略和状态价值\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "for i in range(grid_size):\n",
    "    row = [optimal_v[(i, j)] for j in range(grid_size)]\n",
    "    print(\"  \".join([f\"{val:.1f}\" for val in row]))\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "action_symbols = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→', 'still': '⏺'}\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        s = (i, j)\n",
    "        if s in terminals:\n",
    "            row.append('G')      # 目标\n",
    "        elif s in forbiddens:\n",
    "            row.append('D')      # 危险\n",
    "        else:\n",
    "            best_action = [a for a, prob in optimal_policy[s].items() if prob == 1.0][0]\n",
    "            row.append(action_symbols[best_action])\n",
    "    print(\"  \".join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
